{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Latex\n",
    "from litgpt.scripts.convert_pretrained_checkpoint import convert_pretrained_checkpoint\n",
    "\n",
    "sys.path.append(str(Path(\"src\", \"litgpt\", \"eval\").resolve()))\n",
    "\n",
    "# from lm_eval_harness import run_eval_harness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate the checkpoint\n",
    "\n",
    "Below we define the path to the checkpoint file saved by the training script. You can include the path to the job, e.g., `/teamspace/jobs/MY_JOB_NAME/share/results/CHECKPOINT_NAME.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoint_dir = Path(\"results/final\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the checkpoint if needed\n",
    "\n",
    "The checkpoint saved by the training script contains the weights, but the evaluation harness will also require the model config and tokenizer config to generate the outputs. We can take these from the checkpoint folder of the initial base model and store them in one folder which we then pass to the evaluation harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing results/final/lit_model.pth\n",
      "Saving converted checkpoint to results/export/final/lit_model.pth.\n"
     ]
    }
   ],
   "source": [
    "# Where the converted checkpoint should be saved\n",
    "converted_checkpoint_dir = Path(\"results/export\", pretrained_checkpoint_dir.name)\n",
    "\n",
    "if not converted_checkpoint_dir.exists():\n",
    "    convert_pretrained_checkpoint(checkpoint_dir=pretrained_checkpoint_dir, output_dir=converted_checkpoint_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate samples from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model 'results/export/final/lit_model.pth' with {'name': 'tiny-llama-1.1b', 'hf_config': {'name': 'TinyLlama-1.1B-intermediate-step-1431k-3T', 'org': 'TinyLlama'}, 'scale_embeddings': False, 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 22, 'n_head': 32, 'head_size': 64, 'n_embd': 2048, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 4, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 5632, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'rope_n_elem': 64}\n",
      "Time to instantiate model: 0.09 seconds.\n",
      "Time to load the model weights: 11.47 seconds.\n",
      "Seed set to 1234\n",
      "Time for inference 1: 5.17 sec total, 49.50 tokens/sec\n",
      "Memory used: 2.28 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Proof. Let γ ∈ G be a generating set of G. We claim that γ is a generating set for H. Let xH, yH, zH. We show that xH = x, yH = y, and zH = z. If xH = yH, then yH = zH and x = y = z. Since x x y ∈ G, then x H = y H = y and x H = x. Since x H = y H, then x ∈ (y H) = (H ) ∈ G and x y ∈ H. Since (H ) = H ∈ G and x H = y H, then x y ∈ H. By [29], 8X+P2≤X+P. The following result is from [29]. ► Let H be a subgroup of G. Then, G has an element of order dividing 2P(G )+1 if and only if the Sylow 2-subgroups of G are cyclic. Proof. Let P be a Sylow 2-subgroup of G. If there is an xH of order P(G ), then xH = xz for some z\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"Examination\"\n",
    "temperature = 0.8\n",
    "max_new_tokens = 256\n",
    "seed = 1234\n",
    "\n",
    "\n",
    "out = subprocess.check_output(\n",
    "    [\n",
    "        \"litgpt\", \"generate\", \"base\", \n",
    "        \"--checkpoint_dir\", converted_checkpoint_dir, \n",
    "        \"--prompt\", prompt, \n",
    "        \"--max_new_tokens\", str(max_new_tokens),\n",
    "        \"--temperature\", str(temperature),\n",
    "        \"--seed\", str(seed),\n",
    "    ],\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "display(Latex(out))\n",
    "# print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"In the following radiology report, classify the patient's current microcalcification status as Positive, Negative or Not Stated. BILATERAL SCREENING MAMMOGRAPHY.\n",
    "History: Screening.\n",
    "Comparison available dating from 01/2023.\n",
    "Findings:\n",
    "There are scattered fibroglandular densities bilaterally. No skin thickening or nipple retraction is seen. No grouped calcifications are identified. No spiculated or circumscribed masses are seen.\n",
    "IMPRESSION:\n",
    "No mammographic evidence of malignancy. BI-RADS Category 1.\"\"\"\n",
    "temperature = 0.8\n",
    "max_new_tokens = 10\n",
    "seed = 1234\n",
    "\n",
    "\n",
    "out = subprocess.check_output(\n",
    "    [\n",
    "        \"litgpt\", \"generate\", \"base\", \n",
    "        \"--checkpoint_dir\", converted_checkpoint_dir, \n",
    "        \"--prompt\", prompt, \n",
    "        \"--max_new_tokens\", str(max_new_tokens),\n",
    "        \"--temperature\", str(temperature),\n",
    "        \"--seed\", str(seed),\n",
    "    ],\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "display(Latex(out))\n",
    "# print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation harness\n",
    "\n",
    "This will download the benchmark datasets and use the GPU to run the evaluation on the selected tasks. Make sure you don't already have something running on the GPU, otherwise you may run out of memory. Depending on the selected tasks, this will take between ~20 minutes and 1.5 hours to finish (see progress bar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model 'results/export/final.pth/lit_model.pth' with {'name': 'tiny-llama-1.1b', 'hf_config': {'org': 'TinyLlama', 'name': 'TinyLlama-1.1B-intermediate-step-955k-token-2T'}, 'block_size': 2048, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 22, 'n_head': 32, 'n_embd': 2048, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 4, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 5632, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'head_size': 64, 'rope_n_elem': 64}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tasks: ['piqa', 'boolq']\n",
      "Task: piqa; number of docs: 1838\n",
      "Task: piqa; document 0; context prompt (starting on next line):\n",
      "Question: Remove seeds from  strawberries\n",
      "Answer:\n",
      "(end of prompt on previous line)\n",
      "Requests: [Req_loglikelihood('Question: Remove seeds from  strawberries\\nAnswer:', ' Blend the strawberries, pour the mixture through a fine-mesh strainer with a bowl underneath to catch the pulps and strain out the seeds')[0]\n",
      ", Req_loglikelihood('Question: Remove seeds from  strawberries\\nAnswer:', ' Chop up the strawberries, pour the mixture through a fine-mesh strainer with a bowl underneath to catch the pulps and strain out the seeds')[0]\n",
      "]\n",
      "Task: boolq; number of docs: 3270\n",
      "Task: boolq; document 0; context prompt (starting on next line):\n",
      "NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\n",
      "Question: is ncis new orleans over for the season?\n",
      "Answer:\n",
      "(end of prompt on previous line)\n",
      "Requests: (Req_loglikelihood('NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\\nQuestion: is ncis new orleans over for the season?\\nAnswer:', ' yes')[0]\n",
      ", Req_loglikelihood('NCIS: New Orleans (season 4) -- The fourth season of NCIS: New Orleans premiered on September 26, 2017 on CBS. The series continues to air following Bull, Tuesday at 10:00 p.m. (ET) and contained 24 episodes. The season concluded on May 15, 2018.\\nQuestion: is ncis new orleans over for the season?\\nAnswer:', ' no')[0]\n",
      ")\n",
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10216/10216 [04:03<00:00, 41.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to 'results/evaluation/final.pth/hellaswag-openbookqa-winogrande-arc_easy-arc_challenge-boolq-piqa.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Where the outputs of the eval harness will be saved\n",
    "# eval_dir = Path(\"results/evaluation\", converted_checkpoint_dir.name)\n",
    "\n",
    "# # Where the results will be saved\n",
    "# results_file = eval_dir / f\"{'-'.join(eval_tasks)}.json\"\n",
    "\n",
    "# # The names of the tasks to evaluate over\n",
    "# eval_tasks = [\n",
    "#     \"hellaswag\",\n",
    "#     \"openbookqa\",\n",
    "#     \"winogrande\", \n",
    "#     \"arc_easy\",\n",
    "#     \"arc_challenge\", \n",
    "#     \"boolq\", \n",
    "#     \"piqa\",\n",
    "#     # \"gsm8k\",\n",
    "# ]\n",
    "\n",
    "# run_eval_harness(\n",
    "#     checkpoint_dir=converted_checkpoint_dir,\n",
    "#     eval_tasks=eval_tasks,\n",
    "#     save_filepath=results_file,\n",
    "#     # If you want to do a \"quick\" run on a subset of the datasets, set a number here\n",
    "#     limit=None,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here we load the JSON results file that was saved by the evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'piqa': {'acc': 0.720348204570185, 'acc_stderr': 0.010471899530306562, 'acc_norm': 0.7159956474428727, 'acc_norm_stderr': 0.010521147542454217}, 'boolq': {'acc': 0.6042813455657492, 'acc_stderr': 0.008552742471459795}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boolq</th>\n",
       "      <th>piqa</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.43</td>\n",
       "      <td>71.60</td>\n",
       "      <td>66.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   boolq  piqa   avg\n",
       "0  60.43 71.60 66.01"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# with open(results_file, \"r\") as file:\n",
    "#     results = json.load(file)[\"results\"]\n",
    "\n",
    "# print(results)\n",
    "\n",
    "# table_data = {task_name: [] for task_name in eval_tasks}\n",
    "# average = 0\n",
    "# for task_name, task_results in results.items():\n",
    "#     acc_key = \"acc_norm\" if \"acc_norm\" in task_results else \"acc\"\n",
    "#     acc = task_results[acc_key] * 100\n",
    "#     table_data[task_name] = [acc]\n",
    "#     average += acc\n",
    "\n",
    "# table_data[\"avg\"] = average / len(eval_tasks)\n",
    "\n",
    "# df = pd.DataFrame(table_data)\n",
    "# pd.set_option('display.float_format', lambda x: f'{x:.2f}')\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
